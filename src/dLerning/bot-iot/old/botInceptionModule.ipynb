{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luizp/projects/pibit/pibit/lib64/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple\n",
    "from datetime import datetime as dt\n",
    "import torch.nn.functional as F\n",
    "import random \n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1701\n",
    "EPOCHS = 20\n",
    "MODEL_REPO = \"/home/luiz/pibit/src/cleaner/botClean.csv\"\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToImage:\n",
    "    def __call__(self, array: torch.Tensor, keep_normalization=True):\n",
    "        \"\"\" The idea is to convert a 1d array to a 2d array by resizing (with padding) to the square root of the 1d shape\n",
    "\n",
    "        Ex: \n",
    "            - shape: 2048  \n",
    "            - sqrt(shape) = 45.25 -> round to ceil (46)\n",
    "            - resize the feature vector to 46x46 \n",
    "            - return the new feature vector as a RGB PIL Image for torchvision transforms\n",
    "         \"\"\"\n",
    "        #feat = array.shape[0]\n",
    "        feat = array.shape[0]\n",
    "        n = int(np.ceil(feat ** 0.5))\n",
    "\n",
    "        array = array.cpu().numpy().copy()\n",
    "        \n",
    "        # Squared size with padding\n",
    "        array.resize((n, n))\n",
    "        if not keep_normalization:\n",
    "            return (array * 255).astype(np.uint8)\n",
    "\n",
    "        return torch.Tensor(array.astype(np.float32)).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\" Custom dataset class used for applying transforms to the features. \"\"\"\n",
    "    def __init__(self, subset: Tuple[torch.Tensor, torch.Tensor], transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.subset[0][index], self.subset[1][index]\n",
    "    \n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        return x, y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.subset[0].size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(device: str, epoch: int, loss_fn, MODEL, dataset: DataLoader):\n",
    "    # Validation\n",
    "    MODEL.eval()\n",
    "    it_eval = tqdm(enumerate(dataset), total=len(dataset))\n",
    "    running_loss = 0.\n",
    "    correct = 0\n",
    "    qt = 1\n",
    "    metrics = dict(tp=0, tn=0, fp=0, fn=0)\n",
    "    y_pred = list()\n",
    "    y_true = list()\n",
    "    with torch.no_grad():\n",
    "        for _, (x, y) in it_eval:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            output = MODEL(x)\n",
    "            running_loss += loss_fn(output, y).item()\n",
    "            y_pred.extend(torch.argmax(output, 1).cpu().numpy())\n",
    "            y_true.extend(y.data.cpu().numpy())\n",
    "            correct += torch.sum(torch.argmax(output, 1).eq(y)).item()\n",
    "            qt += len(x)\n",
    "            desc = f\"[{now()}] Epoch {str(epoch).zfill(3)} Val. Acc: {correct/qt:.4f} Val. Loss: {running_loss / len(dataset):.8f}\"\n",
    "            it_eval.set_description(desc)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    metrics[\"tp\"] = tp\n",
    "    metrics[\"fp\"] = fp\n",
    "    metrics[\"tn\"] = tn\n",
    "    metrics[\"fn\"] = fn\n",
    "    return running_loss / len(dataset), correct/qt, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def now():\n",
    "    return dt.now().strftime(\"%d-%m-%Y %H-%M-%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(device: str, epoch: int, optimizer, loss_fn, MODEL, dataset: DataLoader):\n",
    "    # Put the MODEL in the training mode\n",
    "    MODEL.train()\n",
    "    running_loss = 0.\n",
    "    qt = 1\n",
    "    correct = 0\n",
    "\n",
    "    # Just add a fancy progress bar to the terminal...\n",
    "    it = tqdm(enumerate(dataset), total=len(dataset))\n",
    "\n",
    "    for _, (x, y) in it:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        # Make predictions for this batch\n",
    "        outputs = MODEL(x)\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(outputs, y)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "        correct += torch.sum(torch.argmax(outputs, 1).eq(y)).item()\n",
    "        qt += len(x)\n",
    "    \n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        desc = f\"[{now()}] Epoch {str(epoch).zfill(3)} Acc: {correct/qt:.4f} Loss: {running_loss / len(dataset):.8f}\"\n",
    "        it.set_description(desc)\n",
    "    \n",
    "    # Loss / Accuracy\n",
    "    return running_loss / len(dataset), correct/qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionModule(nn.Module):\n",
    "    def __init__(self, in_channels, out1x1, red3x3, out3x3, red5x5, out5x5, pool_proj):\n",
    "        super(InceptionModule, self).__init__()\n",
    "\n",
    "        # 1x1 convolution branch\n",
    "        self.branch1x1 = nn.Conv2d(in_channels, out1x1, kernel_size=1)\n",
    "\n",
    "        # 1x1 convolution followed by 3x3 convolution branch\n",
    "        self.branch3x3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, red3x3, kernel_size=1),\n",
    "            nn.Conv2d(red3x3, out3x3, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "        # 1x1 convolution followed by 5x5 convolution branch\n",
    "        self.branch5x5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, red5x5, kernel_size=1),\n",
    "            nn.Conv2d(red5x5, out5x5, kernel_size=5, padding=2)\n",
    "        )\n",
    "\n",
    "        # 3x3 max pooling followed by 1x1 convolution branch\n",
    "        self.branch_pool = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(in_channels, pool_proj, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "        branch3x3 = self.branch3x3(x)\n",
    "        branch5x5 = self.branch5x5(x)\n",
    "        branch_pool = self.branch_pool(x)\n",
    "\n",
    "        # Concatenate the branches along the channel dimension\n",
    "        outputs = [branch1x1, branch3x3, branch5x5, branch_pool]\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "class GoogleNet(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(GoogleNet, self).__init__()\n",
    "\n",
    "        # Initial convolution layers\n",
    "        self.conv1 = nn.Conv2d(1, 128, kernel_size=7, stride=2, padding=3)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Inception modules\n",
    "        self.inception1 = InceptionModule(128, 128, 128, 128, 32, 32, 32)\n",
    "        self.inception2 = InceptionModule(320, 128, 192, 192, 96, 96, 64)\n",
    "\n",
    "        # ...\n",
    "\n",
    "        # You can add more Inception modules as needed\n",
    "\n",
    "        # Final fully connected layer\n",
    "        self.fc = nn.Linear(480, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.inception1(x)\n",
    "        x = self.inception2(x)\n",
    "\n",
    "        # ...\n",
    "\n",
    "        # You can add more Inception modules as needed\n",
    "\n",
    "        # Global average pooling\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Final fully connected layer\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_update():\n",
    "    # Reproducibility\n",
    "    torch.manual_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    random.seed(SEED)\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(device)\n",
    "\n",
    "    MODEL = GoogleNet().to('cuda')\n",
    "\n",
    "    #if MODEL is None or not isinstance(nn.Module):\n",
    "    #    raise TypeError(\"The model does not exist or isn't an instance of nn.Module (PyTorch)\")\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        ToImage(),\n",
    "    ])\n",
    "    csv_path = \"/home/luizp/projects/pibit/src/cleaner/botClean.csv\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    y = df.iloc[:, -1].values\n",
    "    x = df.iloc[:, :-1].values\n",
    "    y = y[::-1]\n",
    "    x = np.squeeze(x)\n",
    "    #x = x.reshape((1, -1))\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x, y, test_size=0.1, random_state=42\n",
    "    )\n",
    "\n",
    "    #raise Exception(\"Precisa carregar e separar (treino, validação e teste) um dataset qualquer para executar\")\n",
    "    x_train = torch.Tensor(x_train)\n",
    "    y_train = torch.LongTensor(y_train)\n",
    "    x_test  = torch.Tensor(x_test)\n",
    "    y_test  = torch.LongTensor(y_test)\n",
    "    #x_val   = torch.Tensor(x_val)\n",
    "    #y_val   = torch.LongTensor(y_val)\n",
    "\n",
    "    # A parte de fazer o reshape está quando passo uma transformada (ToImage()) como parâmetro\n",
    "    cd_train = CustomDataset(subset=(x_train, y_train), transform=transform)\n",
    "    cd_test  = CustomDataset(subset=(x_test, y_test), transform=transform)\n",
    "    #cd_val   = CustomDataset(subset=(X_val, y_val), transform=transform)\n",
    "\n",
    "    data_train = DataLoader(cd_train, shuffle=True, batch_size=BATCH_SIZE, num_workers=8)\n",
    "    data_test  = DataLoader(cd_test, shuffle=True, batch_size=BATCH_SIZE, num_workers=8)\n",
    "    #data_val   = DataLoader(cd_val, shuffle=True, batch_size=BATCH_SIZE, num_workers=8)\n",
    "\n",
    "    loss_fn   = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(MODEL.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        train_loss, train_acc = train(device, epoch, optimizer, loss_fn, MODEL, data_train)\n",
    "        #val_loss, val_acc, _ = validate(device, epoch, optimizer, loss_fn, MODEL, data_val)\n",
    "\n",
    "        _, _, metrics = validate(device, epoch, loss_fn, MODEL, data_test)\n",
    "        print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[27-01-2024 20-11-50] Epoch 001 Acc: 0.9991 Loss: 0.00748617: 100%|██████████| 3516/3516 [02:55<00:00, 20.07it/s]\n",
      "[27-01-2024 20-11-56] Epoch 001 Val. Acc: 0.9990 Val. Loss: 0.00462034: 100%|██████████| 391/391 [00:05<00:00, 70.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tp': 0, 'tn': 49949, 'fp': 0, 'fn': 51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[27-01-2024 20-14-03] Epoch 002 Acc: 0.9991 Loss: 0.00425109: 100%|██████████| 3516/3516 [02:05<00:00, 27.95it/s]\n",
      "[27-01-2024 20-14-08] Epoch 002 Val. Acc: 0.9990 Val. Loss: 0.00334182: 100%|██████████| 391/391 [00:05<00:00, 70.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tp': 10, 'tn': 49942, 'fp': 7, 'fn': 41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[27-01-2024 20-16-15] Epoch 003 Acc: 0.9991 Loss: 0.00338266: 100%|██████████| 3516/3516 [02:05<00:00, 27.94it/s]\n",
      "[27-01-2024 20-16-20] Epoch 003 Val. Acc: 0.9990 Val. Loss: 0.00212452: 100%|██████████| 391/391 [00:05<00:00, 70.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tp': 10, 'tn': 49942, 'fp': 7, 'fn': 41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[27-01-2024 20-18-27] Epoch 004 Acc: 0.9991 Loss: 0.00303254: 100%|██████████| 3516/3516 [02:05<00:00, 27.93it/s]\n",
      "[27-01-2024 20-18-33] Epoch 004 Val. Acc: 0.9990 Val. Loss: 0.00454989: 100%|██████████| 391/391 [00:05<00:00, 69.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tp': 10, 'tn': 49942, 'fp': 7, 'fn': 41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[27-01-2024 20-20-39] Epoch 005 Acc: 0.9989 Loss: 0.00761102: 100%|██████████| 3516/3516 [02:06<00:00, 27.90it/s]\n",
      "[27-01-2024 20-20-45] Epoch 005 Val. Acc: 0.9951 Val. Loss: 0.00998548: 100%|██████████| 391/391 [00:05<00:00, 70.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tp': 51, 'tn': 49705, 'fp': 244, 'fn': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[27-01-2024 20-22-51] Epoch 006 Acc: 0.9991 Loss: 0.00357015: 100%|██████████| 3516/3516 [02:05<00:00, 27.92it/s]\n",
      "[27-01-2024 20-22-57] Epoch 006 Val. Acc: 0.9991 Val. Loss: 0.00245289: 100%|██████████| 391/391 [00:05<00:00, 69.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tp': 13, 'tn': 49942, 'fp': 7, 'fn': 38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[27-01-2024 20-25-03] Epoch 007 Acc: 0.9992 Loss: 0.00270111: 100%|██████████| 3516/3516 [02:05<00:00, 27.93it/s]\n",
      "[27-01-2024 20-25-09] Epoch 007 Val. Acc: 0.9995 Val. Loss: 0.00178373: 100%|██████████| 391/391 [00:05<00:00, 69.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tp': 37, 'tn': 49937, 'fp': 12, 'fn': 14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[27-01-2024 20-27-16] Epoch 008 Acc: 0.9993 Loss: 0.00210574: 100%|██████████| 3516/3516 [02:05<00:00, 27.96it/s]\n",
      "[27-01-2024 20-27-22] Epoch 008 Val. Acc: 0.9992 Val. Loss: 0.00169288: 100%|██████████| 391/391 [00:05<00:00, 69.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tp': 13, 'tn': 49949, 'fp': 0, 'fn': 38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[27-01-2024 20-29-28] Epoch 009 Acc: 0.9993 Loss: 0.00204904: 100%|██████████| 3516/3516 [02:05<00:00, 27.95it/s]\n",
      "[27-01-2024 20-29-34] Epoch 009 Val. Acc: 0.9991 Val. Loss: 0.00176063: 100%|██████████| 391/391 [00:05<00:00, 70.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tp': 15, 'tn': 49942, 'fp': 7, 'fn': 36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[27-01-2024 20-31-40] Epoch 010 Acc: 0.9993 Loss: 0.00202486: 100%|██████████| 3516/3516 [02:05<00:00, 27.98it/s]\n",
      "[27-01-2024 20-31-46] Epoch 010 Val. Acc: 0.9990 Val. Loss: 0.00246553: 100%|██████████| 391/391 [00:05<00:00, 69.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tp': 3, 'tn': 49949, 'fp': 0, 'fn': 48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[27-01-2024 20-33-52] Epoch 011 Acc: 0.9992 Loss: 0.00193459: 100%|██████████| 3516/3516 [02:05<00:00, 27.97it/s]\n",
      "[27-01-2024 20-33-58] Epoch 011 Val. Acc: 0.9992 Val. Loss: 0.00168834: 100%|██████████| 391/391 [00:05<00:00, 70.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tp': 29, 'tn': 49932, 'fp': 17, 'fn': 22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[27-01-2024 20-36-04] Epoch 012 Acc: 0.9993 Loss: 0.00190241: 100%|██████████| 3516/3516 [02:05<00:00, 27.96it/s]\n",
      "[27-01-2024 20-36-10] Epoch 012 Val. Acc: 0.9994 Val. Loss: 0.00223346: 100%|██████████| 391/391 [00:05<00:00, 70.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tp': 51, 'tn': 49919, 'fp': 30, 'fn': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[27-01-2024 20-38-16] Epoch 013 Acc: 0.9994 Loss: 0.00165865: 100%|██████████| 3516/3516 [02:05<00:00, 27.95it/s]\n",
      "[27-01-2024 20-38-22] Epoch 013 Val. Acc: 0.9992 Val. Loss: 0.00137703: 100%|██████████| 391/391 [00:05<00:00, 70.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tp': 19, 'tn': 49942, 'fp': 7, 'fn': 32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[27-01-2024 20-40-29] Epoch 014 Acc: 0.9993 Loss: 0.00175704: 100%|██████████| 3516/3516 [02:06<00:00, 27.77it/s]\n",
      "[27-01-2024 20-40-35] Epoch 014 Val. Acc: 0.9997 Val. Loss: 0.00152472: 100%|██████████| 391/391 [00:05<00:00, 70.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tp': 49, 'tn': 49937, 'fp': 12, 'fn': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[27-01-2024 20-42-41] Epoch 015 Acc: 0.9994 Loss: 0.00146693: 100%|██████████| 3516/3516 [02:05<00:00, 27.93it/s]\n",
      "[27-01-2024 20-42-47] Epoch 015 Val. Acc: 0.9992 Val. Loss: 0.00206390: 100%|██████████| 391/391 [00:05<00:00, 67.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tp': 13, 'tn': 49949, 'fp': 0, 'fn': 38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[27-01-2024 20-44-54] Epoch 016 Acc: 0.9994 Loss: 0.00184162: 100%|██████████| 3516/3516 [02:06<00:00, 27.84it/s]\n",
      "[27-01-2024 20-45-00] Epoch 016 Val. Acc: 1.0000 Val. Loss: 0.00084072: 100%|██████████| 391/391 [00:05<00:00, 69.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tp': 50, 'tn': 49949, 'fp': 0, 'fn': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[27-01-2024 20-47-06] Epoch 017 Acc: 0.9994 Loss: 0.00158890: 100%|██████████| 3516/3516 [02:05<00:00, 27.99it/s]\n",
      "[27-01-2024 20-47-12] Epoch 017 Val. Acc: 0.9996 Val. Loss: 0.00094188: 100%|██████████| 391/391 [00:05<00:00, 69.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tp': 38, 'tn': 49942, 'fp': 7, 'fn': 13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[27-01-2024 20-49-18] Epoch 018 Acc: 0.9994 Loss: 0.00145545: 100%|██████████| 3516/3516 [02:05<00:00, 27.96it/s]\n",
      "[27-01-2024 20-49-24] Epoch 018 Val. Acc: 0.9995 Val. Loss: 0.00104858: 100%|██████████| 391/391 [00:05<00:00, 69.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tp': 29, 'tn': 49949, 'fp': 0, 'fn': 22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[27-01-2024 20-51-30] Epoch 019 Acc: 0.9994 Loss: 0.00156632: 100%|██████████| 3516/3516 [02:05<00:00, 27.97it/s]\n",
      "[27-01-2024 20-51-36] Epoch 019 Val. Acc: 0.9992 Val. Loss: 0.00357564: 100%|██████████| 391/391 [00:05<00:00, 69.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tp': 13, 'tn': 49949, 'fp': 0, 'fn': 38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[27-01-2024 20-53-42] Epoch 020 Acc: 0.9994 Loss: 0.00149355: 100%|██████████| 3516/3516 [02:05<00:00, 27.98it/s]\n",
      "[27-01-2024 20-53-48] Epoch 020 Val. Acc: 0.9985 Val. Loss: 0.00331326: 100%|██████████| 391/391 [00:05<00:00, 69.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tp': 35, 'tn': 49889, 'fp': 60, 'fn': 16}\n",
      "Finished experiment!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    no_update()\n",
    "    print(\"Finished experiment!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GoogleNet(\n",
      "  (conv1): Conv2d(1, 128, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
      "  (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (inception1): InceptionModule(\n",
      "    (branch1x1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (branch3x3): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (branch5x5): Sequential(\n",
      "      (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    )\n",
      "    (branch_pool): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "      (1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (inception2): InceptionModule(\n",
      "    (branch1x1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (branch3x3): Sequential(\n",
      "      (0): Conv2d(256, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (branch5x5): Sequential(\n",
      "      (0): Conv2d(256, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    )\n",
      "    (branch_pool): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "      (1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=1024, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create an instance of the GoogleNet model\n",
    "model = GoogleNet()\n",
    "\n",
    "# Print the model architecture\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
