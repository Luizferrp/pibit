{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luizp/projects/pibit/pibit/lib64/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple\n",
    "from datetime import datetime as dt\n",
    "import torch.nn.functional as F\n",
    "import random \n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1701\n",
    "EPOCHS = 20\n",
    "MODEL_REPO = \"/home/luizp/projects/pibit/src/cleaner/nslClean.csv\"\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToImage:\n",
    "    def __call__(self, array: torch.Tensor, keep_normalization=True):\n",
    "        \"\"\" The idea is to convert a 1d array to a 2d array by resizing (with padding) to the square root of the 1d shape\n",
    "\n",
    "        Ex: \n",
    "            - shape: 2048  \n",
    "            - sqrt(shape) = 45.25 -> round to ceil (46)\n",
    "            - resize the feature vector to 46x46 \n",
    "            - return the new feature vector as a RGB PIL Image for torchvision transforms\n",
    "         \"\"\"\n",
    "        #feat = array.shape[0]\n",
    "        feat = array.shape[0]\n",
    "        n = int(np.ceil(feat ** 0.5))\n",
    "\n",
    "        array = array.cpu().numpy().copy()\n",
    "        \n",
    "        # Squared size with padding\n",
    "        array.resize((n, n))\n",
    "        if not keep_normalization:\n",
    "            return (array * 255).astype(np.uint8)\n",
    "\n",
    "        return torch.Tensor(array.astype(np.float32)).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\" Custom dataset class used for applying transforms to the features. \"\"\"\n",
    "    def __init__(self, subset: Tuple[torch.Tensor, torch.Tensor], transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.subset[0][index], self.subset[1][index]\n",
    "    \n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        return x, y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.subset[0].size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(device: str, epoch: int, loss_fn, MODEL, dataset: DataLoader):\n",
    "    # Validation\n",
    "    MODEL.eval()\n",
    "    it_eval = tqdm(enumerate(dataset), total=len(dataset))\n",
    "    running_loss = 0.\n",
    "    correct = 0\n",
    "    qt = 1\n",
    "    metrics = dict(tp=0, tn=0, fp=0, fn=0)\n",
    "    y_pred = list()\n",
    "    y_true = list()\n",
    "    with torch.no_grad():\n",
    "        for _, (x, y) in it_eval:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            output = MODEL(x)\n",
    "            running_loss += loss_fn(output, y).item()\n",
    "            y_pred.extend(torch.argmax(output, 1).cpu().numpy())\n",
    "            y_true.extend(y.data.cpu().numpy())\n",
    "            correct += torch.sum(torch.argmax(output, 1).eq(y)).item()\n",
    "            qt += len(x)\n",
    "            desc = f\"[{now()}] Epoch {str(epoch).zfill(3)} Val. Acc: {correct/qt:.4f} Val. Loss: {running_loss / len(dataset):.8f}\"\n",
    "            it_eval.set_description(desc)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    metrics[\"tp\"] = tp\n",
    "    metrics[\"fp\"] = fp\n",
    "    metrics[\"tn\"] = tn\n",
    "    metrics[\"fn\"] = fn\n",
    "    return running_loss / len(dataset), correct/qt, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def now():\n",
    "    return dt.now().strftime(\"%d-%m-%Y %H-%M-%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(device: str, epoch: int, optimizer, loss_fn, MODEL, dataset: DataLoader):\n",
    "    # Put the MODEL in the training mode\n",
    "    MODEL.train()\n",
    "    running_loss = 0.\n",
    "    qt = 1\n",
    "    correct = 0\n",
    "\n",
    "    # Just add a fancy progress bar to the terminal...\n",
    "    it = tqdm(enumerate(dataset), total=len(dataset))\n",
    "\n",
    "    for _, (x, y) in it:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        # Make predictions for this batch\n",
    "        outputs = MODEL(x)\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(outputs, y)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "        correct += torch.sum(torch.argmax(outputs, 1).eq(y)).item()\n",
    "        qt += len(x)\n",
    "    \n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        desc = f\"[{now()}] Epoch {str(epoch).zfill(3)} Acc: {correct/qt:.4f} Loss: {running_loss / len(dataset):.8f}\"\n",
    "        it.set_description(desc)\n",
    "    \n",
    "    # Loss / Accuracy\n",
    "    return running_loss / len(dataset), correct/qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeparableConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(SeparableConv2d, self).__init__()\n",
    "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding, groups=in_channels, bias=False)\n",
    "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, reps, stride=1):\n",
    "        super(Block, self).__init__()\n",
    "\n",
    "        # Residual connection\n",
    "        self.residual = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "        layers = []\n",
    "        for _ in range(reps):\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            layers.append(SeparableConv2d(out_channels, out_channels))\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "\n",
    "        self.block = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.residual(x)\n",
    "        x = self.block(x)\n",
    "        return x + residual\n",
    "\n",
    "class Xception(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(Xception, self).__init__()\n",
    "\n",
    "        # Entry flow\n",
    "        self.entry_flow = nn.Sequential(\n",
    "            nn.Conv2d(1, 128, kernel_size=7, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, 128, kernel_size=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            Block(128, 128, reps=2, stride=2),\n",
    "            Block(128, 256, reps=2, stride=2),\n",
    "            Block(256, 728, reps=2, stride=2),\n",
    "        )\n",
    "\n",
    "        # Middle flow\n",
    "        self.middle_flow = nn.Sequential(\n",
    "            Block(728, 728, reps=3),\n",
    "            Block(728, 728, reps=3),\n",
    "            Block(728, 728, reps=3),\n",
    "        )\n",
    "\n",
    "        # Exit flow\n",
    "        self.exit_flow = nn.Sequential(\n",
    "            Block(728, 728, reps=2),\n",
    "            SeparableConv2d(728, 1024, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            SeparableConv2d(1024, 1536, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(1536),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            SeparableConv2d(1536, 2048, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(2048),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2048, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.entry_flow(x)\n",
    "        x = self.middle_flow(x)\n",
    "        x = self.exit_flow(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_update():\n",
    "    # Reproducibility\n",
    "    torch.manual_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    random.seed(SEED)\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(device)\n",
    "\n",
    "    MODEL = Xception().to('cuda')\n",
    "\n",
    "    #if MODEL is None or not isinstance(nn.Module):\n",
    "    #    raise TypeError(\"The model does not exist or isn't an instance of nn.Module (PyTorch)\")\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        ToImage(),\n",
    "    ])\n",
    "    csv_path = \"/home/luizp/projects/pibit/src/cleaner/nslClean.csv\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    y = df.iloc[:, -1].values\n",
    "    x = df.iloc[:, :-1].values\n",
    "    y = y[::-1]\n",
    "    x = np.squeeze(x)\n",
    "    #x = x.reshape((1, -1))\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x, y, test_size=0.1, random_state=42\n",
    "    )\n",
    "\n",
    "    #raise Exception(\"Precisa carregar e separar (treino, validação e teste) um dataset qualquer para executar\")\n",
    "    x_train = torch.Tensor(x_train)\n",
    "    y_train = torch.LongTensor(y_train)\n",
    "    x_test  = torch.Tensor(x_test)\n",
    "    y_test  = torch.LongTensor(y_test)\n",
    "    #x_val   = torch.Tensor(x_val)\n",
    "    #y_val   = torch.LongTensor(y_val)\n",
    "\n",
    "    # A parte de fazer o reshape está quando passo uma transformada (ToImage()) como parâmetro\n",
    "    cd_train = CustomDataset(subset=(x_train, y_train), transform=transform)\n",
    "    cd_test  = CustomDataset(subset=(x_test, y_test), transform=transform)\n",
    "    #cd_val   = CustomDataset(subset=(X_val, y_val), transform=transform)\n",
    "\n",
    "    data_train = DataLoader(cd_train, shuffle=True, batch_size=BATCH_SIZE, num_workers=8)\n",
    "    data_test  = DataLoader(cd_test, shuffle=True, batch_size=BATCH_SIZE, num_workers=8)\n",
    "    #data_val   = DataLoader(cd_val, shuffle=True, batch_size=BATCH_SIZE, num_workers=8)\n",
    "\n",
    "    loss_fn   = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(MODEL.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        train_loss, train_acc = train(device, epoch, optimizer, loss_fn, MODEL, data_train)\n",
    "        #val_loss, val_acc, _ = validate(device, epoch, optimizer, loss_fn, MODEL, data_val)\n",
    "\n",
    "        _, _, metrics = validate(device, epoch, loss_fn, MODEL, data_test)\n",
    "        print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/886 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/luizp/projects/pibit/src/deep/kddSqueezeNet.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/luizp/projects/pibit/src/deep/kddSqueezeNet.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/luizp/projects/pibit/src/deep/kddSqueezeNet.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     no_update()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/luizp/projects/pibit/src/deep/kddSqueezeNet.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFinished experiment!\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/home/luizp/projects/pibit/src/deep/kddSqueezeNet.ipynb Cell 10\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/luizp/projects/pibit/src/deep/kddSqueezeNet.ipynb#X13sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(MODEL\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mLEARNING_RATE)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/luizp/projects/pibit/src/deep/kddSqueezeNet.ipynb#X13sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, EPOCHS \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/luizp/projects/pibit/src/deep/kddSqueezeNet.ipynb#X13sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m train(device, epoch, optimizer, loss_fn, MODEL, data_train)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/luizp/projects/pibit/src/deep/kddSqueezeNet.ipynb#X13sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     \u001b[39m#val_loss, val_acc, _ = validate(device, epoch, optimizer, loss_fn, MODEL, data_val)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/luizp/projects/pibit/src/deep/kddSqueezeNet.ipynb#X13sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     _, _, metrics \u001b[39m=\u001b[39m validate(device, epoch, loss_fn, MODEL, data_test)\n",
      "\u001b[1;32m/home/luizp/projects/pibit/src/deep/kddSqueezeNet.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/luizp/projects/pibit/src/deep/kddSqueezeNet.ipynb#X13sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/luizp/projects/pibit/src/deep/kddSqueezeNet.ipynb#X13sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# Make predictions for this batch\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/luizp/projects/pibit/src/deep/kddSqueezeNet.ipynb#X13sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m outputs \u001b[39m=\u001b[39m MODEL(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/luizp/projects/pibit/src/deep/kddSqueezeNet.ipynb#X13sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# Zero your gradients for every batch!\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/luizp/projects/pibit/src/deep/kddSqueezeNet.ipynb#X13sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/projects/pibit/pibit/lib64/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/projects/pibit/pibit/lib64/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/home/luizp/projects/pibit/src/deep/kddSqueezeNet.ipynb Cell 10\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/luizp/projects/pibit/src/deep/kddSqueezeNet.ipynb#X13sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/luizp/projects/pibit/src/deep/kddSqueezeNet.ipynb#X13sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mentry_flow(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/luizp/projects/pibit/src/deep/kddSqueezeNet.ipynb#X13sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmiddle_flow(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/luizp/projects/pibit/src/deep/kddSqueezeNet.ipynb#X13sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexit_flow(x)\n",
      "File \u001b[0;32m~/projects/pibit/pibit/lib64/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/projects/pibit/pibit/lib64/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/pibit/pibit/lib64/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/projects/pibit/pibit/lib64/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/projects/pibit/pibit/lib64/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/home/luizp/projects/pibit/src/deep/kddSqueezeNet.ipynb Cell 10\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/luizp/projects/pibit/src/deep/kddSqueezeNet.ipynb#X13sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m residual \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresidual(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/luizp/projects/pibit/src/deep/kddSqueezeNet.ipynb#X13sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblock(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/luizp/projects/pibit/src/deep/kddSqueezeNet.ipynb#X13sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x \u001b[39m+\u001b[39;49m residual\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    no_update()\n",
    "    print(\"Finished experiment!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
