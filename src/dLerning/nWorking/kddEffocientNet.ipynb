{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luizp/projects/pibit/pibit/lib64/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple\n",
    "from datetime import datetime as dt\n",
    "import torch.nn.functional as F\n",
    "import random \n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1701\n",
    "EPOCHS = 20\n",
    "MODEL_REPO = \"/home/luizp/projects/pibit/src/data/processed/csv/nslClean.csv\"\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToImage:\n",
    "    def __call__(self, array: torch.Tensor, keep_normalization=True):\n",
    "        \"\"\" The idea is to convert a 1d array to a 2d array by resizing (with padding) to the square root of the 1d shape\n",
    "\n",
    "        Ex: \n",
    "            - shape: 2048  \n",
    "            - sqrt(shape) = 45.25 -> round to ceil (46)\n",
    "            - resize the feature vector to 46x46 \n",
    "            - return the new feature vector as a RGB PIL Image for torchvision transforms\n",
    "         \"\"\"\n",
    "        #feat = array.shape[0]\n",
    "        feat = array.shape[0]\n",
    "        n = int(np.ceil(feat ** 0.5))\n",
    "\n",
    "        array = array.cpu().numpy().copy()\n",
    "        \n",
    "        # Squared size with padding\n",
    "        array.resize((n, n))\n",
    "        if not keep_normalization:\n",
    "            return (array * 255).astype(np.uint8)\n",
    "\n",
    "        return torch.Tensor(array.astype(np.float32)).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\" Custom dataset class used for applying transforms to the features. \"\"\"\n",
    "    def __init__(self, subset: Tuple[torch.Tensor, torch.Tensor], transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.subset[0][index], self.subset[1][index]\n",
    "    \n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        return x, y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.subset[0].size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(device: str, epoch: int, loss_fn, MODEL, dataset: DataLoader):\n",
    "    # Validation\n",
    "    MODEL.eval()\n",
    "    it_eval = tqdm(enumerate(dataset), total=len(dataset))\n",
    "    running_loss = 0.\n",
    "    correct = 0\n",
    "    qt = 1\n",
    "    metrics = dict(tp=0, tn=0, fp=0, fn=0)\n",
    "    y_pred = list()\n",
    "    y_true = list()\n",
    "    with torch.no_grad():\n",
    "        for _, (x, y) in it_eval:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            output = MODEL(x)\n",
    "            running_loss += loss_fn(output, y).item()\n",
    "            y_pred.extend(torch.argmax(output, 1).cpu().numpy())\n",
    "            y_true.extend(y.data.cpu().numpy())\n",
    "            correct += torch.sum(torch.argmax(output, 1).eq(y)).item()\n",
    "            qt += len(x)\n",
    "            desc = f\"[{now()}] Epoch {str(epoch).zfill(3)} Val. Acc: {correct/qt:.4f} Val. Loss: {running_loss / len(dataset):.8f}\"\n",
    "            it_eval.set_description(desc)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    metrics[\"tp\"] = tp\n",
    "    metrics[\"fp\"] = fp\n",
    "    metrics[\"tn\"] = tn\n",
    "    metrics[\"fn\"] = fn\n",
    "    return running_loss / len(dataset), correct/qt, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def now():\n",
    "    return dt.now().strftime(\"%d-%m-%Y %H-%M-%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(device: str, epoch: int, optimizer, loss_fn, MODEL, dataset: DataLoader):\n",
    "    # Put the MODEL in the training mode\n",
    "    MODEL.train()\n",
    "    running_loss = 0.\n",
    "    qt = 1\n",
    "    correct = 0\n",
    "\n",
    "    # Just add a fancy progress bar to the terminal...\n",
    "    it = tqdm(enumerate(dataset), total=len(dataset))\n",
    "\n",
    "    for _, (x, y) in it:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        # Make predictions for this batch\n",
    "        outputs = MODEL(x)\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(outputs, y)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "        correct += torch.sum(torch.argmax(outputs, 1).eq(y)).item()\n",
    "        qt += len(x)\n",
    "    \n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        desc = f\"[{now()}] Epoch {str(epoch).zfill(3)} Acc: {correct/qt:.4f} Loss: {running_loss / len(dataset):.8f}\"\n",
    "        it.set_description(desc)\n",
    "    \n",
    "    # Loss / Accuracy\n",
    "    return running_loss / len(dataset), correct/qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=4):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_channels, in_channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_channels // reduction, in_channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y\n",
    "\n",
    "class MBConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, expand_ratio, stride, kernel_size, se_ratio=0.25):\n",
    "        super(MBConvBlock, self).__init__()\n",
    "        self.stride = stride\n",
    "        self.use_residual = self.stride == 1 and in_channels == out_channels\n",
    "\n",
    "        hidden_dim = in_channels * expand_ratio\n",
    "        self.expand = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, hidden_dim, 1, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            Swish()\n",
    "        )\n",
    "\n",
    "        self.depthwise = nn.Sequential(\n",
    "            nn.Conv2d(hidden_dim, hidden_dim, kernel_size, stride, kernel_size // 2, groups=hidden_dim, bias=False),\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            Swish()\n",
    "        )\n",
    "\n",
    "        if se_ratio:\n",
    "            se_channels = int(in_channels * se_ratio)\n",
    "            self.se = SEBlock(hidden_dim, se_channels)\n",
    "        else:\n",
    "            self.se = None\n",
    "\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv2d(hidden_dim, out_channels, 1, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.expand(x)\n",
    "        y = self.depthwise(y)\n",
    "\n",
    "        if self.se:\n",
    "            y = self.se(y)\n",
    "\n",
    "        y = self.project(y)\n",
    "\n",
    "        if self.use_residual:\n",
    "            return x + y\n",
    "        else:\n",
    "            return y\n",
    "\n",
    "class EfficientNet(nn.Module):\n",
    "    def __init__(self, num_classes=2, width_coefficient=1.0, depth_coefficient=1.0, dropout_rate=0.2):\n",
    "        super(EfficientNet, self).__init__()\n",
    "\n",
    "        # Define the architecture parameters\n",
    "        self.width_coefficient = width_coefficient\n",
    "        self.depth_coefficient = depth_coefficient\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # Define the base number of channels\n",
    "        base_channels = 128\n",
    "\n",
    "        # Define the network architecture\n",
    "        channels = int(128 * width_coefficient)\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, channels, 3, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(channels),\n",
    "            Swish()\n",
    "        )\n",
    "\n",
    "        # MBConv Blocks\n",
    "        mbconv_settings = [\n",
    "            # t, c, n, s, k, se\n",
    "            [1, 24, 1, 1, 3, 0.25],\n",
    "            [6, 24, 2, 2, 3, 0.25],\n",
    "            [6, 40, 2, 2, 5, 0.25],\n",
    "            [6, 40, 3, 2, 3, 0.25],\n",
    "            [6, 112, 3, 1, 5, 0.25],\n",
    "            [6, 112, 4, 2, 5, 0.25],\n",
    "            [6, 144, 4, 4, 3, 0.25],\n",
    "        ]\n",
    "\n",
    "        #for t, c, n, s, k, se in mbconv_settings:\n",
    "        #    out_channels = int(c * width_coefficient)\n",
    "        #    layers = []\n",
    "        #    for i in range(n):\n",
    "        #        stride = s if i == 0 else 1\n",
    "        #        layers.append(MBConvBlock(channels, out_channels, t, stride, k, se))\n",
    "        #        channels = out_channels\n",
    "        #    setattr(self, f'mbconv{n}', nn.Sequential(*layers))\n",
    "        for i, (t, c, n, s, k, se) in enumerate(mbconv_settings, 1):\n",
    "            out_channels = int(c * width_coefficient)\n",
    "            layers = []\n",
    "            for _ in range(n):\n",
    "                stride = s if i == 1 else 1  # For the first block in each setting, use the specified stride\n",
    "                layers.append(MBConvBlock(channels, out_channels, t, stride, k, se))\n",
    "                channels = out_channels\n",
    "            setattr(self, f'mbconv{i}', nn.Sequential(*layers))\n",
    "        # Head\n",
    "        last_channels = int(1280 * width_coefficient)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(channels, last_channels, 1, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(last_channels),\n",
    "            Swish(),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "        )\n",
    "\n",
    "        # Fully-connected layer\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(last_channels, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.mbconv1(x)\n",
    "        x = self.mbconv2(x)\n",
    "        x = self.mbconv3(x)\n",
    "        x = self.mbconv4(x)\n",
    "        x = self.mbconv5(x)\n",
    "        x = self.mbconv6(x)\n",
    "        x = self.mbconv7(x)\n",
    "        x = self.head(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_update():\n",
    "    # Reproducibility\n",
    "    torch.manual_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    random.seed(SEED)\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(device)\n",
    "\n",
    "    MODEL = EfficientNet().to('cuda')\n",
    "\n",
    "    #if MODEL is None or not isinstance(nn.Module):\n",
    "    #    raise TypeError(\"The model does not exist or isn't an instance of nn.Module (PyTorch)\")\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        ToImage(),\n",
    "    ])\n",
    "    csv_path = \"/home/luizp/projects/pibit/src/data/processed/csv/nslClean.csv\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    y = df.iloc[:, -1].values\n",
    "    x = df.iloc[:, :-1].values\n",
    "    y = y[::-1]\n",
    "    x = np.squeeze(x)\n",
    "    #x = x.reshape((1, -1))\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x, y, test_size=0.1, random_state=42\n",
    "    )\n",
    "\n",
    "    #raise Exception(\"Precisa carregar e separar (treino, validação e teste) um dataset qualquer para executar\")\n",
    "    x_train = torch.Tensor(x_train)\n",
    "    y_train = torch.LongTensor(y_train)\n",
    "    x_test  = torch.Tensor(x_test)\n",
    "    y_test  = torch.LongTensor(y_test)\n",
    "    #x_val   = torch.Tensor(x_val)\n",
    "    #y_val   = torch.LongTensor(y_val)\n",
    "\n",
    "    # A parte de fazer o reshape está quando passo uma transformada (ToImage()) como parâmetro\n",
    "    cd_train = CustomDataset(subset=(x_train, y_train), transform=transform)\n",
    "    cd_test  = CustomDataset(subset=(x_test, y_test), transform=transform)\n",
    "    #cd_val   = CustomDataset(subset=(X_val, y_val), transform=transform)\n",
    "\n",
    "    data_train = DataLoader(cd_train, shuffle=True, batch_size=BATCH_SIZE, num_workers=8)\n",
    "    data_test  = DataLoader(cd_test, shuffle=True, batch_size=BATCH_SIZE, num_workers=8)\n",
    "    #data_val   = DataLoader(cd_val, shuffle=True, batch_size=BATCH_SIZE, num_workers=8)\n",
    "\n",
    "    loss_fn   = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(MODEL.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        train_loss, train_acc = train(device, epoch, optimizer, loss_fn, MODEL, data_train)\n",
    "        #val_loss, val_acc, _ = validate(device, epoch, optimizer, loss_fn, MODEL, data_val)\n",
    "\n",
    "        _, _, metrics = validate(device, epoch, loss_fn, MODEL, data_test)\n",
    "        print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[21-02-2024 09-57-04] Epoch 001 Acc: 0.4938 Loss: 0.08583818:  12%|█▏        | 53/443 [00:43<05:21,  1.21it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/luizp/projects/pibit/src/dLerning/nWorking/kddEffocientNet.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/luizp/projects/pibit/src/dLerning/nWorking/kddEffocientNet.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/luizp/projects/pibit/src/dLerning/nWorking/kddEffocientNet.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     no_update()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/luizp/projects/pibit/src/dLerning/nWorking/kddEffocientNet.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFinished experiment!\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/home/luizp/projects/pibit/src/dLerning/nWorking/kddEffocientNet.ipynb Cell 10\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/luizp/projects/pibit/src/dLerning/nWorking/kddEffocientNet.ipynb#X12sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(MODEL\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mLEARNING_RATE)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/luizp/projects/pibit/src/dLerning/nWorking/kddEffocientNet.ipynb#X12sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, EPOCHS \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/luizp/projects/pibit/src/dLerning/nWorking/kddEffocientNet.ipynb#X12sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m train(device, epoch, optimizer, loss_fn, MODEL, data_train)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/luizp/projects/pibit/src/dLerning/nWorking/kddEffocientNet.ipynb#X12sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     \u001b[39m#val_loss, val_acc, _ = validate(device, epoch, optimizer, loss_fn, MODEL, data_val)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/luizp/projects/pibit/src/dLerning/nWorking/kddEffocientNet.ipynb#X12sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     _, _, metrics \u001b[39m=\u001b[39m validate(device, epoch, loss_fn, MODEL, data_test)\n",
      "\u001b[1;32m/home/luizp/projects/pibit/src/dLerning/nWorking/kddEffocientNet.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/luizp/projects/pibit/src/dLerning/nWorking/kddEffocientNet.ipynb#X12sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/luizp/projects/pibit/src/dLerning/nWorking/kddEffocientNet.ipynb#X12sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(outputs, y)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/luizp/projects/pibit/src/dLerning/nWorking/kddEffocientNet.ipynb#X12sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/luizp/projects/pibit/src/dLerning/nWorking/kddEffocientNet.ipynb#X12sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# Adjust learning weights\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/luizp/projects/pibit/src/dLerning/nWorking/kddEffocientNet.ipynb#X12sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/projects/pibit/pibit/lib64/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/projects/pibit/pibit/lib64/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    no_update()\n",
    "    print(\"Finished experiment!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
